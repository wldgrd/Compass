# Objetivo  
O objetivo √© praticar a combina√ß√£o de conhecimentos vistos no programa e fazer um mix de tudo que j√° foi dito.  

# Perguntas  

Aqui ser√£o analisadas perguntas referentes aos filmes dos g√™neros crime e/ou guerra. Em rela√ß√£o √† Sprint anterior, houve mudan√ßas de algumas perguntas para a an√°lise. 

1. **Qual √© a distribui√ß√£o da nota m√©dia dos filmes por g√™nero crime/guerra nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Verificar a prefer√™ncia do p√∫blico em rela√ß√£o a cada um dos g√™neros citados.  

2. **Como o tempo de dura√ß√£o dos filmes se relaciona com a nota m√©dia e com o n√∫mero de votos nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Perceber se existe alguma prefer√™ncia por filmes mais longos ou mais curtos.  

3. **Qual √© o filme de cada g√™nero com maior n√∫mero de votos das √∫ltimas 5 d√©cadas?**
**Motivo da an√°lise:** Verificar se possuem artistas em comum e o enredo dos filmes de maior sucesso para entender se existe uma correla√ß√£o.  

4. **Qual √© a propor√ß√£o de filmes war/crime nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Entender se a prefer√™ncia por um g√™nero se mant√©m ou se h√° oscila√ß√µes de prefer√™ncia. Caso haja, qual o motivo para tais oscila√ß√µes?  

5. **Qual(is) √© (s√£o) o(s) pa√≠s(es) que mais produz(em) filmes de cada g√™nero nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Verificar uma poss√≠vel fonte e parcerias entre produtoras de pa√≠ses estrangeiros com pa√≠ses que mais produzem filmes desses g√™neros.  

6. **Qual √© a principal l√≠ngua falada em cada g√™nero nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Avaliar se existe uma tend√™ncia ou prefer√™ncia por uma determinada l√≠ngua.  

#

# Instru√ß√µes Gerais  
**Processamento da camada Trusted:**
a camada trusted de um data lake corresponde √†quela em que os dados encontram-se limpos e s√£o confi√°veis. 
√â resultado da integra√ß√£o das diversas fontes de origem, que encontram-se na camada anterior, que chamamos Raw.

Aqui faremos o uso do **apache spark** no processo, atrav√©s do servi√ßo **AWS Glue**, integrando dados existentes na camada 
Raw Zone para a Trusted Zone.  
O objetivo √© gerar uma vis√£o padronizada dos dados, persistida no S3, dispon√≠vel num database do Glue Data Catalog e acess√≠vel via **AWS Athena** compreendendo a **Trusted Zone** do data lake.  
Assim, todos os dados da Trusted Zone possuem o mesmo formato de armazenamento e **todos podem ser analisados no 
AWS Athena atrav√©s de comandos SQL.**

Todos os dados ser√£o persistidos na Trusted no formato **PARQUET**, particionados por data de cria√ß√£o do arquivo 
no momento da ingest√£o do dado do TMDB. Deve-se considerar o padr√£o:

**\origem do dado\formato do dado\especifica√ß√£o do dado\data de ingest√£o separada por ano\mes\dia\arquivo.**  
**Exemplo:**
```
s3://desafio-final-pb-welder/Trusted/TMDB/Parquet/movies/Ano/Mes/Dia/movies.parquet  

s3://desafio-final-pb-welder/Trusted/Local/Parquet/movies/movies.parquet
```

A exce√ß√£o fica para os dados oriundos do processamento batch (CSV), que n√£o precisam ser particionados.

Todos os jobs Spark ser√£o criados por meio do AWS Glue. Iremos separar o processamento em **dois jobs**:   
- O primeiro, ser√° respons√°vel pelo processamento dos arquivos CSV e o segundo pelo processamento dos dados  

- Oriundos da API TMDB. Lembre-se que suas origens ser√£o os dados existentes na RAW Zone.

**N√£o use notebooks do Glue.**

Desenvolva os Jobs no Glue utilizando a op√ß√£o spark script editor. Ap√≥s, na aba job details, atente para as seguintes op√ß√µes:
- worker type: informe G 1x (op√ß√£o de menor configura√ß√£o)
- requested number of workers: informe 2, que √© a quantidade m√≠nima.
- job timeout (minutes): mantenha 60 ou menos, se poss√≠vel.


# C√≥digos e Execu√ß√£o  

**Processamento do arquivo CSV**  
```python
import sys
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
import pyspark.sql.functions as F
import pyspark.sql.types as T
from awsglue.utils import getResolvedOptions

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

source_file = args['S3_INPUT_PATH']
target_path = args['S3_TARGET_PATH']

# Defini√ß√£o dos tipos das colunas
schema = T.StructType([
    T.StructField("id", T.StringType(), True),
    T.StructField("tituloPincipal", T.StringType(), True),  
    T.StructField("tituloOriginal", T.StringType(), True),
    T.StructField("anoLancamento", T.IntegerType(), True),
    T.StructField("tempoMinutos", T.IntegerType(), True),  # Pode conter valores "\N"
    T.StructField("genero", T.StringType(), True),
    T.StructField("notaMedia", T.DoubleType(), True),
    T.StructField("numeroVotos", T.IntegerType(), True),
    T.StructField("generoArtista", T.StringType(), True),
    T.StructField("personagem", T.StringType(), True),
    T.StructField("nomeArtista", T.StringType(), True),
    T.StructField("anoNascimento", T.StringType(), True),  # Pode conter valores "\N"
    T.StructField("anoFalecimento", T.StringType(), True),  # Pode conter valores "\N"
    T.StructField("profissao", T.StringType(), True),
    T.StructField("titulosMaisConhecidos", T.StringType(), True)
])

# Lendo o arquivo CSV
df = spark.read.csv(source_file, schema=schema, sep='|', header=True)

# Corrigindo o nome da coluna
df = df.withColumnRenamed("tituloPincipal", "tituloPrincipal")

# Exibindo as 10 primeiras linhas no log
df.show(10)

# Escrevendo no formato Parquet
#df.write.mode("overwrite").parquet(target_path)
df.coalesce(1).write.mode("overwrite").parquet(target_path)

# Finalizando o job
job.commit()
```  

**Processamento dos JSON**  
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext

# Obter argumentos do Glue
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])
input_path = args['S3_INPUT_PATH']
target_path = args['S3_TARGET_PATH']

# Criar contexto do Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Ler todos os arquivos JSON da pasta
df = spark.read.option("multiline", "true").json(input_path)

# Salvar como Parquet
df.write.mode("overwrite").parquet(target_path)

print(f"Arquivos Parquet salvos em: {target_path}")

job.commit()
```
**OBS:** no processamento dos arquivos JSON optei por n√£o definir o schema devido a problemas de processamento que encontrei. Sendo assim, o spark fez a infer√™ncia e o que precisar ser alterado, ser√° feito via script no momento da an√°lise dos dados.  

# Resultados das Execu√ß√µes  
**JOBS GLUE**
![desafio1](../Evidencias/desafio1.png)  
#
**Crawler**
![desafio2](../Evidencias/desafio2.png)  
#
**Configura√ß√£o do crawler**
![desafio3](../Evidencias/desafio3.png)  
#
**Database Filmes Gerado**
![desafio4](../Evidencias/desafio4.png)  
#
**Tabelas**
![desafio5](../Evidencias/desafio5.png)  
#
**Athena - Query para verifica√ß√£o dos dados oriundos do CSV**
![desafio6](../Evidencias/desafio6.png)  
#
**Athena - Query para verifica√ß√£o dos dados oriundos dos JSON**
![desafio7](../Evidencias/desafio7.png)  
#

# Links
[üìú**Certificados**](/Sprint8/Certificados/)  
[üïµÔ∏è‚Äç‚ôÇÔ∏è**Evid√™ncias** ](/Sprint8/Evidencias/)  
[üí™**Exerc√≠cios**](/Sprint8/Exercicios/)  
[üñ≥ **Desafio**](/Sprint8/Desafio/README.md)  