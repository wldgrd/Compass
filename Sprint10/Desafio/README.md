# Objetivo  
O objetivo √© praticar a combina√ß√£o de conhecimentos vistos no programa e fazer um mix de tudo que j√° foi dito.  

# Perguntas  

Aqui ser√£o analisadas perguntas referentes aos filmes dos g√™neros crime e/ou guerra. 

1. **Qual √© a distribui√ß√£o da nota m√©dia dos filmes por g√™nero crime/guerra nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Verificar a prefer√™ncia do p√∫blico em rela√ß√£o a cada um dos g√™neros citados.  

2. **Como o tempo de dura√ß√£o dos filmes se relaciona com a nota m√©dia e com o n√∫mero de votos nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Perceber se existe alguma prefer√™ncia por filmes mais longos ou mais curtos.  

3. **Qual √© o filme de cada g√™nero com maior n√∫mero de votos das √∫ltimas 5 d√©cadas?**
**Motivo da an√°lise:** Verificar se possuem artistas em comum e o enredo dos filmes de maior sucesso para entender se existe uma correla√ß√£o.  

4. **Qual √© a propor√ß√£o de filmes war/crime nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Entender se a prefer√™ncia por um g√™nero se mant√©m ou se h√° oscila√ß√µes de prefer√™ncia. Caso haja, qual o motivo para tais oscila√ß√µes?  

5. **Qual(is) √© (s√£o) o(s) pa√≠s(es) que mais produz(em) filmes de cada g√™nero nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Verificar uma poss√≠vel fonte e parcerias entre produtoras de pa√≠ses estrangeiros com pa√≠ses que mais produzem filmes desses g√™neros.  

6. **Qual √© a principal l√≠ngua falada em cada g√™nero nas √∫ltimas 5 d√©cadas?**  
**Motivo da an√°lise:** Avaliar se existe uma tend√™ncia ou prefer√™ncia por uma determinada l√≠ngua.  

#

# Instru√ß√µes Gerais  
**Processamento da camada Refined:**
A camada refined de um datalake corresponde √† camada onde os dados est√£o prontos e tratados para consumo. 
√â resultado da integra√ß√£o das diversas fontes de origem, que encontram-se na camada anterior, que chamamos Trusted.

Aqui faremos o uso do **apache spark** no processo, atrav√©s do servi√ßo **AWS Glue**, integrando dados existentes na camada 
Trusted Zone para a Refined Zone.  
O objetivo √© gerar uma vis√£o padronizada dos dados, persistida no S3, dispon√≠vel num database do Glue Data Catalog e acess√≠vel via **AWS Athena** compreendendo a **Refined Zone** do data lake.  
Assim, todos os dados da Refined Zone possuem o mesmo formato de armazenamento e **todos podem ser analisados no 
AWS Athena atrav√©s de comandos SQL.**


O job spark foi criado atrav√©s do AWS Glue e tem como objetivo criar o modelo dimensional dos dados que ser√£o usados na an√°lise para responder √†s perguntas citadas anteriormente. 

Primeiramente, o modelo a ser criado est√° ilustrado na figura a seguir: 
![modelo dimensional](../Evidencias/modelo_dimensional.png)  

Esse modelo tem como objetivo separar os dados em tabelas menores (dimens√µes) interligadas pela tabela fato. Para cada tabela, existe um ou mais campos de chave prim√°ria simples ou composta.

Vale ressaltar que nem todos os dados ser√£o usados na an√°lise, por√©m decidi criar as tabelas com dados a mais para que, em caso futuro, a an√°lise seja facilmente expans√≠vel.  

# C√≥digos e Execu√ß√£o  
O c√≥digo do job executado no glue pode ser encontrado na √≠ntegra clicando [aqui](../Desafio/processamento_refined.py)
 
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext
from pyspark.sql.functions import explode, col
from awsglue.dynamicframe import DynamicFrame

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_INPUT_PATH', 'S3_TARGET_PATH'])
input_path = args['S3_INPUT_PATH']
target_path = args['S3_TARGET_PATH']

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)
```  
No trecho acima s√£o importadas as bibliotecas necess√°rias e tamb√©m configurados os par√¢metros de vari√°vel ambiente 'S3_INPUT_PATH' e 'S3_TARGET_PATH'. Em S3_INPUT_PATH o caminho √© direcionado para a trusted zone (s3://desafio-final-pb-welder/Trusted/TMDB/) e em S3_TARGET_PATH o caminho √© direcionado para o diret√≥rio Refined (s3://desafio-final-pb-welder/Refined/) onde ser√£o criadas as tabelas via execu√ß√£o do Crawler.  

```python
#Criar Tabela Fato (fato_filmes)
fato_filmes = df.select(
    "id", "imdb_id", "budget", "popularity", "release_date", "runtime", "vote_average", "vote_count"
)

#Criar Dimens√£o G√™neros (dim_generos)
dim_generos = df.withColumn("genre", explode(col("genres")))
dim_generos = dim_generos.selectExpr("id as movie_id", "genre.id as genre_id", "genre.name as genre_name")

#Criar Dimens√£o Elenco (dim_elenco)
dim_elenco = df.withColumn("cast_member", explode(col("cast")))
dim_elenco = dim_elenco.selectExpr(
    "id as movie_id", "cast_member.id as cast_id", "cast_member.name as name", 
    "cast_member.character as character", "cast_member.birthday as birthday", 
    "cast_member.deathday as deathday", "cast_member.gender as gender", 
    "cast_member.popularity as popularity"
)

#Criar Dimens√£o Empresas (dim_empresas)
dim_empresas = df.withColumn("company", explode(col("production_companies")))
dim_empresas = dim_empresas.selectExpr("id as movie_id", "company.id as company_id", "company.name as company_name", "company.origin_country as company_country")

#Criar Dimens√£o Pa√≠ses (dim_pa√≠ses)
dim_paises = df.withColumn("country", explode(col("production_countries")))
dim_paises = dim_paises.selectExpr("id as movie_id", "country.iso_3166_1 as iso_3166_1", "country.name as country_name")

#Criar Dimens√£o Idiomas (dim_idiomas)
dim_idiomas = df.withColumn("language", explode(col("spoken_languages")))
dim_idiomas = dim_idiomas.selectExpr("id as movie_id", "language.iso_639_1 as iso_639_1", "language.english_name as english_name", "language.name as name")

#Salvar tabelas no S3 para uso no Glue Data Catalog
fato_filmes.write.mode("overwrite").parquet(f"{target_path}/fato_filmes")
dim_generos.write.mode("overwrite").parquet(f"{target_path}/dim_generos")
dim_elenco.write.mode("overwrite").parquet(f"{target_path}/dim_elenco")
dim_empresas.write.mode("overwrite").parquet(f"{target_path}/dim_empresas")
dim_paises.write.mode("overwrite").parquet(f"{target_path}/dim_paises")
dim_idiomas.write.mode("overwrite").parquet(f"{target_path}/dim_idiomas")
```  
Ap√≥s a leitura dos arquivos parquet da camada trusted, o script cria dataframes menores que depois ser√£o salvos como tabelas no destino especificado. √â importante ressaltar que em colunas nas quais havia dados aninhados, o comando explode foi utilizado para desacoplar as subcolunas dentro do struct e assim permitindo a cria√ß√£o do modelo.  

O resultado da execu√ß√£o do job foi o conjunto de pastas ilustrado na figura abaixo.  
![bucket](../Evidencias/bucket.png)  

Ap√≥s a execu√ß√£o do job no glue, foi criado um crawler associado conforme ilustrado nas figuras a seguir.  
![crawler](../Evidencias/crawler_1.png)  
![crawler](../Evidencias/crawler_2.png)  

Uma vez que o crawler foi executado com sucesso, foi feita a verifica√ß√£o tabela a tabela via Athena.  
<br>  

**Tabela Fato** 
![fato](../Evidencias/fato_filmes.png)  

**Dimens√£o Elenco** 
![elenco](../Evidencias/dim_elenco.png)  

**Dimens√£o Empresas**  
![empresas](../Evidencias/dim_empresas.png)  

**Dimens√£o G√™neros**  
![g√™neros](../Evidencias/dim_generos.png)  

**Dimens√£o Idiomas**  
![idiomas](../Evidencias/dim_idiomas.png)  

**Dimens√£o Pa√≠ses**  
![pa√≠ses](../Evidencias/dim_paises.png)  

<br> 

**OBS:** a ingest√£o dos dados foi feita baseada nos IDs do arquivo CSV contido na camada RAW. Todos os IDs presentes no arquivo, pertencentes aos g√™neros de crime/guerra foram filtrados e seus dados ingeridos via API do TMDB. Portanto, os dados finais s√£o um compilado das informa√ß√µes de ambas as fontes.  


# Links
[üìú**Certificados**](/Sprint9/Certificados/)  
[üïµÔ∏è‚Äç‚ôÇÔ∏è**Evid√™ncias** ](/Sprint9/Evidencias/)  
[üí™**Exerc√≠cios**](/Sprint9/Exercicios/)  
[üñ≥ **Desafio**](/Sprint9/Desafio/README.md)  